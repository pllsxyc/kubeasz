k8s的系统相对来说比较复杂了，那么使用ansible playbook进行集群管控，这个playbook就比较复杂了，可以把这个项目当做一个很好的playbook参考书，这样在写其他playbook的时候可以做些借鉴
这个文件的作用就是把roles目录下的所有role的功能分步总结。以便在分析流程的时候快速定位。

[chrony]
1、卸载ntp
2、安装chrony，分为离线安装还是在线安装，离线安装对各种系统进行判断安装
3、安装完之后，根据server client分别进行配置、启动


[deploy]
这个role用于在ansible主机上生成一些集群需要的证书、config配置文件,在使用ansible-playbook部署的时候，目标主机是localhost
1、准备一些目录，也就是/etc/ansible/.cluster/{ssl,backup}
2、在/etc/ansible目录下创建bin目录，权限755
3、判断是否存在证书文件来注册变量p，以便重复执行的时候保证幂等性而不再创建证书
4、准备CA配置文件和签名请求，其中就是两个文件： ca-config.json  ca-csr.json
   其中ca-config.json中配置的是50年过期，ca-csr.json配置的是100年过期	其中根据变量p决定是否跳过
5、生成CA证书和私钥 用于给各个组件签名证书 其中根据变量p决定是否跳过
   cfssl gencert -initca ca-csr.json
   cfssljson -bare ca
   这样生成ca的私钥cs-key.pem、ca的证书ca.pem,ca.csr是用于ca请求签名的文件
   2020/08/17 14:56:28 [INFO] generating a new CA key and certificate from CSR
   2020/08/17 14:56:28 [INFO] generate received request
   2020/08/17 14:56:28 [INFO] received CSR
   2020/08/17 14:56:28 [INFO] generating key: rsa-2048
   2020/08/17 14:56:29 [INFO] encoded CSR
   2020/08/17 14:56:29 [INFO] signed certificate with serial number 218990291340890989473094657237962215806466142591
6、配置kubectl的配置文件
	1、删除原有的kubeconfig
	2、如果只读的话，配置只读rbac文件
	3、准备kubectl的USER_NAME证书签名请求文件admin-csr.json
	4、自签名admin的证书还有私钥文件
	cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes admin-csr.json | cfssljson -bare admin"
     上面的步骤生成admin的私钥文件admin-key.pem admin的证书文件admin.pem
    5、设置集群的参数
    /etc/ansible/bin/kubectl config set-cluster {{ CLUSTER_NAME }} \
        --certificate-authority={{ base_dir }}/.cluster/ssl/ca.pem \
        --embed-certs=true \
        --server={{ KUBE_APISERVER }}"
      apiserver 默认是第一个master节点，指定证书机构为自己创建的ca，证书ca.pem，相当于浏览器导入证书一样，
    6、配置客户端认证参数 kubectl操作需要双向认证，这里指定客户端的证书与私钥文件
    kubectl config set-credentials admin \
        --client-certificate=/etc/ansible/.cluster/ssl/admin.pem \
        --embed-certs=true \
        --client-key=/etc/ansible/.cluster/ssl/admin-key.pem"
    7、设置上下文参数
    kubectl config set-context context-cluster1-admin --cluster=cluster1 --user=admin"

    8、选择默认上下文
    kubectl config use-context {{ CONTEXT_NAME }}"
 7、配置kube-proxy
 	1、准备kube-proxy证书签名请求,文件是kube-proxy-csr.json
 	2、创建kube-proxy的证书与私钥
 	{{ base_dir }}/bin/cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes kube-proxy-csr.json | {{ base_dir }}/bin/cfssljson -bare kube-proxy"
    3、设置集群的参数 指定ca的证书文件
    { base_dir }}/bin/kubectl config set-cluster kubernetes \
        --certificate-authority={{ base_dir }}/.cluster/ssl/ca.pem \
        --embed-certs=true \
        --server={{ KUBE_APISERVER }} \
        --kubeconfig={{ base_dir }}/.cluster/kube-proxy.kubeconfig
    4、设置客户端认证参数 指定kube-proxy的私钥和证书文件
    {{ base_dir }}/bin/kubectl config set-credentials kube-proxy \
        --client-certificate={{ base_dir }}/.cluster/ssl/kube-proxy.pem \
        --client-key={{ base_dir }}/.cluster/ssl/kube-proxy-key.pem \
        --embed-certs=true \
        --kubeconfig={{ base_dir }}/.cluster/kube-proxy.kubeconfig
    5、设置上下文以及选择默认的上下文
8、配置kube-controller-manager
	1、创建证书签名签名请求文件 kube-controller-manager-csr.json
	2、签名得到证书与私钥
	3、设置集群参数 指定ca的证书、kube-controller-manager的配置文件
	kubectl config set-cluster kubernetes \
        --certificate-authority={{ base_dir }}/.cluster/ssl/ca.pem \
        --embed-certs=true \
        --server={{ KUBE_APISERVER }} \
        --kubeconfig={{ base_dir }}/.cluster/kube-controller-manager.kubeconfig"
    4、设置认证参数
    kubectl config set-credentials system:kube-controller-manager \
        --client-certificate={{ base_dir }}/.cluster/ssl/kube-controller-manager.pem \
        --client-key={{ base_dir }}/.cluster/ssl/kube-controller-manager-key.pem \
        --embed-certs=true \
        --kubeconfig={{ base_dir }}/.cluster/kube-controller-manager.kubeconfig"
    5、设置上下文与选择默认上下文
 9、配置kube-scheduler
 	具体步骤类似于上面组件的配置
 10、将easzctl放到/usr/bin/里面可以直接用
 11、将kubectl放到/usr/bin/里面可以直接用
 12、配置kubectl自动补全
 13、安装netaddr


[prepare]
1、卸载删除默认的防火墙软件。
2、在线或者离线安装基础软件包，分为：
	bash-completion
	conntrack-tools
	ipset          
	ipvsadm        
	libseccomp     
	nfs-utils      
	psmisc         
	rsync          
	socat       
3、关闭selinux   
4、journal日志相关优化
5、禁用系统swap
6、转换内核版本为浮点数
7、设置NF_CONNTRACK，以系统内核是否大于4.19为依据，用于后面打开的内核模块
8、加载内核的模块，一些关于ipvs的模块
9、启用systemd自动加载模块服务
10、增加内核模块开机加载配置
11、设置一些系统参数，并且立即生效
12、创建systemd配置目录、设置系统的ulimits以及把SCTP列入内核模块黑名单
13、创建k8s相关的二进制文件的存放目录、ca目录、/root/.kube,其中bin_dir、ca_dir 在hosts文件中定义
14、复制证书工具到目标主机
15、配置环境变量PATH，将k8s二进制文件的路径加进去
16、分发证书相关的文件：
	admin.pem
	admin-key.pem
	ca.pem
	ca-key.pem
	ca-config.json
17、添加kubectl自动补全
18、分发/root/.kube/config，这个文件在本机的/root/.kube/config，copy过去即可,这个文件生成于01.prepare.yml的操作中
19、分发/etc/ansible/.cluster/kube-proxy.kubeconfig 到目的主机的/etc/kubernetes/kube-proxy.kubeconfig,类似的还有kube-controller-manager、kube-scheduler的配置文件,只不过这两个文件是只复制到master节点上。


[etcd]
安装etcd的role
1、准备etcd的相关目录：二进制文件存放目录，ca的目录、证书的目录、etcd的工作目录
2、离线copy etcd的二进制文件 etcd、etcdctl
3、分发证书相关文件包括ca.pem、ca-key.pem、ca-config.json
4、创建etc的证书请求文件etcd-csr.json
5、创建etcd的证书以及私钥
6、后面的流程就是启动etcd以及开机启动


[containerd]
因为容器运行时选择的docker，所以这个跳过


[docker]
安装docker的操作role
1、判断是否已经安装过docker并注册变量 docker_svc
2、判断是否安装过containerd并注册变量 
3、判断docker版本
4、准备docker相关目录
5、根据docker版本是否大于18.09 下载不同的docker二进制文件。
6、配置docker命令自动补全
7、配置docker国内加速 daemon.json
8、开启docker以及自启动


[kube-master]
master节点做的一些操作
1、下载kube-master 二进制文件包括
	- kube-apiserver
	- kube-controller-manager
	- kube-scheduler
	- kubectl       
2、创建k8s的证书签名请求文件kubernetes-csr.json
3、创建k8s的证书和私钥
	fssl gencert \
        -ca={{ ca_dir }}/ca.pem \
        -ca-key={{ ca_dir }}/ca-key.pem \
        -config={{ ca_dir }}/ca-config.json \
        -profile=kubernetes kubernetes-csr.json | {{ bin_dir }}/cfssljson -bare kubernetes"

4、创建aggregator proxy证书请求以及创建证书、私钥
5、根据是否开启basic-auth设置随机密码以及创建basic-auth.csv文件,这里通过修改roles/kube-master/defaults/main.yml中设置auth-basic为yes以及自己创建账号密码。
6、替换kubeconfig中的kube-apiserver地址。也就是每个master节点修改kube-apiserver为自己的内网地址。
包括以下文件：
- "/root/.kube/config"
- "/etc/kubernetes/kube-controller-manager.kubeconfig"
- "/etc/kubernetes/kube-scheduler.kubeconfig"
7、创建master服务的systemd unit文件
包括以下文件：
  - kube-apiserver.service
  - kube-controller-manager.service
  - kube-scheduler.service
 8、对上述master节点的服务开机自启动
 9、启动master节点的服务。
 10、配置用于basic-auth的用户的rbac权限。


[kube-node]
node节点所做的操作role
1、创建kube-node的相关目录。包括：
  - /var/lib/kubelet
  - /var/lib/kube-proxy
  - /etc/cni/net.d
2、下载node节点所需的二进制文件
  - kubectl
  - kubelet
  - kube-proxy
  - bridge
  - host-local
  - loopback
3、安装haproxy，其中haproxy中的上游地址是for 循环master节点。而haproxy监听的是127.0.0.1:6443，
4、替换kube-apiserver 的地址，将本地的kube-apiserver 配置成http://127.0.0.1:6443,也就是先访问本地的haproxy服务，然后根据负载均衡算法反向代理到真正的master节点。node节点是这样，而master节点直接访问本机ip地址:6443

5、配置kubelet，包括证书签名请求、证书、私钥、设置集群参数、客户端认证参数、设置上下文参数。
6、准备cni配置文件
7、创建kubelet的配置文件、systemd unit文件
8、开机自启动kubelet、启动kubelet服务。
9、替换kube-proxy配置文件中的apiserver 地址，地址策略和上面一样。
10、创建kube-proxy服务文件，开机自启动、启动kube-proxy
11、轮询等待kubelet启动然后等待node节点达到ready状态。

[flannel]
1、创建flannel相关目录，包括：
	/etc/cni/net.d
	/opt/kube/images
	/opt/kube/kube-system
2、配置flannel daemonset的yaml文件
3、下载flannel cni plugins
  - bridge
  - flannel
  - host-local
  - loopback
  - portmap
4、尝试导入flannel的离线镜像
5、运行flannel网络
6、删除默认的cni配置
7、轮询等待flannel运行。

--------其他网络插件和flannel流程差不多，可以看具体网络插件的role

[cluster-addon]
1、在node节点创建/opt/kube/kube-system目录
2、准备DNS的配置文件
3、获取所有已经创建的pod信息
4、获取已经下载的离线镜像信息
5、推动离线镜像以及导入 包括dns、metrics-server、dashboard
6、分别部署dns、metric-server、dashboard


